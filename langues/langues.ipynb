{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《红楼梦》曹雪芹\n",
      "\n",
      "\n",
      "\n",
      "严正声明：本书为丫丫小说网(www.shuyaya.com)的用户上传至其在本站的存储空间，本站只提供TXT全集电子书存储服务以及免费下载服务，以下作品内容之版权与本站无任何关系。\n",
      "\n",
      "在线阅读：http://www.shuyaya.com/read/2034/\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "第一回  甄士隐梦幻识通灵　贾雨村风尘怀闺秀\n",
      "\n",
      "\n",
      "\n",
      "    \t\t\n",
      "\n",
      "\n",
      "\n",
      "    此开卷第一回也．作者自云：因曾历过一番梦幻之后，故将真事隐去，而借\"通灵\"之说，撰此《石头记》一书也．故曰\"甄士隐\"云云．但书中所记何事何人？自又云：“今风尘碌碌，一事无成，忽念及当日所有之女子，一一细考较去，觉其行止见识，皆出于我之上．何我堂堂须眉，诚不若彼裙钗哉？实愧则有余，悔又无益之大无可如何之日也！当此，则自欲将已往所赖天恩祖德，锦衣纨绔之时，饫甘餍肥之日，背父兄教育之恩，负师友规谈之德，以至今日一技无成，半生潦倒之罪，编述一集，以告天下人：我之罪固不免，然闺阁中本自历历有人，万不可因我之不肖，自护己短，一并使其泯灭也．虽今日之茅椽蓬牖，瓦灶绳床，其晨夕风露，阶柳庭花，亦未有妨我之襟怀笔墨者．虽我未学，下笔无文，又何妨用假语村言，敷演出一段故事来，亦可使闺阁昭传，复可悦世之目，破人愁闷，不亦宜乎？\"故曰\"贾雨村\"云云．\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/hongloumeng.txt') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        print line\n",
    "        i += 1\n",
    "        if i >10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_sentence(text):\n",
    "    text = (text).decode('utf-8')\n",
    "    start = 0\n",
    "    i = 0\n",
    "    sentences = []\n",
    "    punt_list = '.!?~． 。！？～\\n'.decode('utf-8')\n",
    "    for word in text:\n",
    "        if word in punt_list and token not in punt_list:\n",
    "            sentences.append(text[start:i+1])\n",
    "            start = i+1\n",
    "            i +=1\n",
    "        else:\n",
    "            i +=1\n",
    "            token = list(text[start:i+2]).pop()\n",
    "    if start < len(text):\n",
    "        sentences.append(text[start:])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/hongloumeng.txt', 'r')\n",
    "context = f.read()\n",
    "sentences = cut_sentence(context)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《红楼梦》曹雪芹\n",
      "\n",
      "\n",
      "严正声明：本书为丫丫小说网(www.\n",
      "shuyaya.\n",
      "com)的用户上传至其在本站的存储空间，本站只提供TXT全集电子书存储服务以及免费下载服务，以下作品内容之版权与本站无任何关系。\n",
      "\n",
      "在线阅读：http://www.\n",
      "shuyaya.\n",
      "com/read/2034/\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "第一回  \n",
      "甄士隐梦幻识通灵　贾雨村风尘怀闺秀\n",
      "\n",
      "    \n",
      "\t\t\n",
      "\n",
      "    \n",
      "此开卷第一回也．\n",
      "作者自云：因曾历过一番梦幻之后，故将真事隐去，而借\"通灵\"之说，撰此《石头记》一书也．\n",
      "故曰\"甄士隐\"云云．\n",
      "但书中所记何事何人？\n",
      "自又云：“今风尘碌碌，一事无成，忽念及当日所有之女子，一一细考较去，觉其行止见识，皆出于我之上．\n",
      "何我堂堂须眉，诚不若彼裙钗哉？\n",
      "实愧则有余，悔又无益之大无可如何之日也！\n",
      "当此，则自欲将已往所赖天恩祖德，锦衣纨绔之时，饫甘餍肥之日，背父兄教育之恩，负师友规谈之德，以至今日一技无成，半生潦倒之罪，编述一集，以告天下人：我之罪固不免，然闺阁中本自历历有人，万不可因我之不肖，自护己短，一并使其泯灭也．\n",
      "虽今日之茅椽蓬牖，瓦灶绳床，其晨夕风露，阶柳庭花，亦未有妨我之襟怀笔墨者．\n"
     ]
    }
   ],
   "source": [
    "for s in senteces[:20]:\n",
    "    print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_re = re.compile(u'[^\\u4E00-\\u9FA5]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/cleaned_hongloumeng.txt', 'w')\n",
    "for s in sentences:\n",
    "    cleaned_s = filter_re.sub(r'', s)\n",
    "    if len(cleaned_s):\n",
    "        f.write(cleaned_s.encode('utf-8')+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = []\n",
    "for s in sentences:\n",
    "    cleaned_s = filter_re.sub(r'', s)\n",
    "    if len(cleaned_s) :\n",
    "        cleaned_sentences.append(cleaned_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\\u7ea2\\u697c\\u68a6\\u66f9\\u96ea\\u82b9',\n",
       " u'\\u4e25\\u6b63\\u58f0\\u660e\\u672c\\u4e66\\u4e3a\\u4e2b\\u4e2b\\u5c0f\\u8bf4\\u7f51',\n",
       " u'\\u7684\\u7528\\u6237\\u4e0a\\u4f20\\u81f3\\u5176\\u5728\\u672c\\u7ad9\\u7684\\u5b58\\u50a8\\u7a7a\\u95f4\\u672c\\u7ad9\\u53ea\\u63d0\\u4f9b\\u5168\\u96c6\\u7535\\u5b50\\u4e66\\u5b58\\u50a8\\u670d\\u52a1\\u4ee5\\u53ca\\u514d\\u8d39\\u4e0b\\u8f7d\\u670d\\u52a1\\u4ee5\\u4e0b\\u4f5c\\u54c1\\u5185\\u5bb9\\u4e4b\\u7248\\u6743\\u4e0e\\u672c\\u7ad9\\u65e0\\u4efb\\u4f55\\u5173\\u7cfb',\n",
       " u'\\u5728\\u7ebf\\u9605\\u8bfb',\n",
       " u'\\u7b2c\\u4e00\\u56de',\n",
       " u'\\u7504\\u58eb\\u9690\\u68a6\\u5e7b\\u8bc6\\u901a\\u7075\\u8d3e\\u96e8\\u6751\\u98ce\\u5c18\\u6000\\u95fa\\u79c0',\n",
       " u'\\u6b64\\u5f00\\u5377\\u7b2c\\u4e00\\u56de\\u4e5f',\n",
       " u'\\u4f5c\\u8005\\u81ea\\u4e91\\u56e0\\u66fe\\u5386\\u8fc7\\u4e00\\u756a\\u68a6\\u5e7b\\u4e4b\\u540e\\u6545\\u5c06\\u771f\\u4e8b\\u9690\\u53bb\\u800c\\u501f\\u901a\\u7075\\u4e4b\\u8bf4\\u64b0\\u6b64\\u77f3\\u5934\\u8bb0\\u4e00\\u4e66\\u4e5f',\n",
       " u'\\u6545\\u66f0\\u7504\\u58eb\\u9690\\u4e91\\u4e91',\n",
       " u'\\u4f46\\u4e66\\u4e2d\\u6240\\u8bb0\\u4f55\\u4e8b\\u4f55\\u4eba']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache c:\\users\\mefly\\appdata\\local\\temp\\jieba.cache\n",
      "Loading model cost 2.818 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自\n",
      "又\n",
      "云今\n",
      "风尘碌碌\n",
      "一事无成\n",
      "忽\n",
      "念及\n",
      "当日\n",
      "所有\n",
      "之\n",
      "女子\n",
      "一一\n",
      "细考\n",
      "较\n",
      "去\n",
      "觉\n",
      "其\n",
      "行止\n",
      "见识\n",
      "皆\n",
      "出于\n",
      "我\n",
      "之上\n"
     ]
    }
   ],
   "source": [
    "for s in jieba.cut(\"自又云今风尘碌碌一事无成忽念及当日所有之女子一一细考较去觉其行止见识皆出于我之上\"):\n",
    "    print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_sentences = [list(jieba.cut(s)) for s in cleaned_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ngrams = lambda sentence, n : zip(*[sentence[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自 又\n",
      "又 云今\n",
      "云今 风尘碌碌\n",
      "风尘碌碌 一事无成\n",
      "一事无成 忽\n",
      "忽 念及\n",
      "念及 当日\n",
      "当日 所有\n",
      "所有 之\n",
      "之 女子\n",
      "女子 一一\n",
      "一一 细考\n",
      "细考 较\n",
      "较 去\n",
      "去 觉\n",
      "觉 其\n",
      "其 行止\n",
      "行止 见识\n",
      "见识 皆\n",
      "皆 出于\n",
      "出于 我\n",
      "我 之上\n",
      "自\n",
      "又\n",
      "云今\n",
      "风尘碌碌\n",
      "一事无成\n",
      "忽\n",
      "念及\n",
      "当日\n",
      "所有\n",
      "之\n",
      "女子\n",
      "一一\n",
      "细考\n",
      "较\n",
      "去\n",
      "觉\n",
      "其\n",
      "行止\n",
      "见识\n",
      "皆\n",
      "出于\n",
      "我\n",
      "之上\n"
     ]
    }
   ],
   "source": [
    "# test generate_ngrams function\n",
    "for bigram in generate_ngrams(list(jieba.cut(cleaned_sentences[10])), 2):\n",
    "    print bigram[0], bigram[1]\n",
    "for unigram in generate_ngrams(list(jieba.cut(cleaned_sentences[10])), 1):\n",
    "    print unigram[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum likelihood parameter estimation \n",
    "def ngrams_parameter_estimate(sentences, n):\n",
    "    sentences_copy = copy.deepcopy(sentences)\n",
    "    ngrams_dict = {}\n",
    "    num_ngrams = 0\n",
    "    for words in sentences_copy:\n",
    "        for i in range(n-1):\n",
    "            words.insert(0, '*')\n",
    "        words.append('#')\n",
    "        ngrams = generate_ngrams(words, n)\n",
    "        for ngram in ngrams:\n",
    "            ngrams_dict[ngram] = ngrams_dict.get(ngram, 0.0) + 1.0\n",
    "            num_ngrams += 1\n",
    "    \n",
    "    # normalize\n",
    "    #for ngram in ngrams_dict:\n",
    "        #ngrams_dict[ngram] /= num_ngrams\n",
    "        \n",
    "    return ngrams_dict, num_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dict, n_bigrams = ngrams_parameter_estimate(cut_sentences, 1)\n",
    "filtered_unigrams_dict = OrderedDict(sorted([(k, v)  for k, v in unigrams_dict.iteritems() if v >= 10], key = lambda (k, v) : (v, k), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: #, frequency: 34412.0\n",
      "word: 了, frequency: 19613.0\n",
      "word: 今儿, frequency: 286.0\n",
      "word: 一刻, frequency: 10.0\n"
     ]
    }
   ],
   "source": [
    "print 'word: %s, frequency: %s' % (filtered_unigrams_dict.keys()[0][0], filtered_unigrams_dict[filtered_unigrams_dict.keys()[0]])\n",
    "print 'word: %s, frequency: %s' % (filtered_unigrams_dict.keys()[1][0], filtered_unigrams_dict[filtered_unigrams_dict.keys()[1]])\n",
    "print 'word: %s, frequency: %s' % (filtered_unigrams_dict.keys()[200][0], filtered_unigrams_dict[filtered_unigrams_dict.keys()[200]])\n",
    "print 'word: %s, frequency: %s' % (filtered_unigrams_dict.keys()[-1][0], filtered_unigrams_dict[filtered_unigrams_dict.keys()[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'frequency')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.distplot(np.log10(filtered_unigrams_dict.values()))\n",
    "ax.set_xlabel(r'$\\log(word frequency)$')\n",
    "ax.set_ylabel(r'frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train, Valid, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(cut_sentences)\n",
    "n_valid = int(n_samples * 0.2)\n",
    "n_test = int(n_samples * 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_sentences = np.array(cut_sentences)[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shuffled_sentences[:n_samples-n_test-n_valid]\n",
    "valid = shuffled_sentences[n_samples-n_test-n_valid:n_samples-n_test]\n",
    "test = shuffled_sentences[n_samples-n_test:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Language Model on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_dict, n_unigrams = ngrams_parameter_estimate(train, 1)\n",
    "bigrams_dict, n_bigrams = ngrams_parameter_estimate(train, 2)\n",
    "trigrams_dict, n_trigrams = ngrams_parameter_estimate(train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Our Uni-, Bi-, Tri-gram Model on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_log_probability(sentence, n, num_grams, ngrams_dict, n_1grams_dict=None):\n",
    "    assert len(ngrams_dict.keys()[0]) == n, n\n",
    "    if n_1grams_dict is None:\n",
    "        assert n == 1\n",
    "    ngrams = generate_ngrams(sentence, n)\n",
    "    log_prob, has_unknown_ngram = 0.0, False\n",
    "    for ngram in ngrams:\n",
    "        if ngram in ngrams_dict:\n",
    "            if n == 1:\n",
    "                log_prob += np.log2(ngrams_dict[ngram]/num_grams)\n",
    "            else:\n",
    "                log_prob += np.log2(ngrams_dict[ngram]/n_1grams_dict[ngram[:n-1]])\n",
    "        else:\n",
    "            has_unknown_ngram = True\n",
    "    return 0.0 if has_unknown_ngram else log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_words(sentences):\n",
    "    \n",
    "    return np.sum([len(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words_in_test = compute_total_words(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134443"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words_in_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in test: 134443\n",
      "Perplexity for uni gram language model: 14.9899421316\n",
      "Perplexity for bi gram language model: 1.08565685362\n",
      "Perplexity for tri gram language model: 1.00741697138\n"
     ]
    }
   ],
   "source": [
    "ngrams_list = [unigram_dict, bigrams_dict, trigrams_dict]\n",
    "ngrams_names = ['uni', 'bi', 'tri']\n",
    "print \"Number of words in test: {}\".format(n_words_in_test)\n",
    "for n in range(1, 4):\n",
    "    sum_log_prob = 0.0\n",
    "    for s in test:\n",
    "        if n == 1:\n",
    "            sum_log_prob += calculate_sentence_log_probability(s, n, n_unigrams, ngrams_list[n-1])\n",
    "        else:\n",
    "            sum_log_prob += calculate_sentence_log_probability(s, n, n_unigrams, ngrams_list[n-1], ngrams_list[n-2])\n",
    "    perplexity = 2**(-sum_log_prob/n_words_in_test)\n",
    "    \n",
    "    print \"Perplexity for {} gram language model: {}\".format(ngrams_names[n-1], perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in valid: 90255\n",
      "Perplexity for uni gram language model: 15.3983164967\n",
      "Perplexity for bi gram language model: 1.08717603287\n",
      "Perplexity for tri gram language model: 1.00737080073\n"
     ]
    }
   ],
   "source": [
    "n_words_in_valid = compute_total_words(valid)\n",
    "print \"Number of words in valid: {}\".format(n_words_in_valid)\n",
    "for n in range(1, 4):\n",
    "    sum_log_prob = 0.0\n",
    "    for s in valid:\n",
    "        if n == 1:\n",
    "            sum_log_prob += calculate_sentence_log_probability(s, n, n_unigrams, ngrams_list[n-1])\n",
    "        else:\n",
    "            sum_log_prob += calculate_sentence_log_probability(s, n, n_unigrams, ngrams_list[n-1], ngrams_list[n-2])\n",
    "    perplexity = 2**(-sum_log_prob/n_words_in_valid)\n",
    "    \n",
    "    print \"Perplexity for {} gram language model: {}\".format(ngrams_names[n-1], perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Interpolation Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_1 = 0.2#lambda for unigram\n",
    "lambda_2 = 0.3#lambda for bigram\n",
    "lambda_3 = 0.5#lambda for trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log_probability_with_linear_interpolation(sentence, num_grams, ngrams_list, lambdas):\n",
    "    ngrams = generate_ngrams(sentence, 3)\n",
    "    log_prob, has_unknown_ngram = 0.0, False\n",
    "    for ngram in ngrams:\n",
    "        if ngram in ngrams_list[2]:\n",
    "            prob = 0.0\n",
    "            for i in range(3):\n",
    "                if i == 0:\n",
    "                    prob += ngrams_list[i][ngram[:i+1]]*lambdas[i]/num_grams\n",
    "                else:\n",
    "                    prob += ngrams_list[i][ngram[:i+1]]*lambdas[i]/ngrams_list[i-1][ngram[:i]]\n",
    "            log_prob += np.log2(prob) if prob > 0.0 else 0.0\n",
    "        else:\n",
    "            has_unknown_ngram = True\n",
    "    return 0.0 if has_unknown_ngram else log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in valid: 90255\n",
      "Perplexity for smoothed trigram language model: 1.00830438637\n"
     ]
    }
   ],
   "source": [
    "n_words_in_valid = compute_total_words(valid)\n",
    "lambdas = [lambda_1, lambda_2, lambda_3]\n",
    "print \"Number of words in valid: {}\".format(n_words_in_valid)\n",
    "sum_log_prob = 0.0\n",
    "for s in valid:\n",
    "    sum_log_prob += sentence_log_probability_with_linear_interpolation(s, n_unigrams, ngrams_list, lambdas)\n",
    "perplexity = 2**(-sum_log_prob/n_words_in_valid)\n",
    "    \n",
    "print \"Perplexity for smoothed trigram language model: {}\".format(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_lambdas(sentences, n_iter, n_unigrams, ngrams_list, verbose=True):\n",
    "    best_lambda_1, best_lambda_2, best_lambda_3 = None, None, None\n",
    "    best_log_prob = -np.inf\n",
    "    for i in range(n_iter):\n",
    "        lambda_1 = np.random.uniform(0, 1)\n",
    "        lambda_2 = np.random.uniform(0, 1)\n",
    "        lambda_3 = 1.0 - lambda_1 - lambda_2\n",
    "        lambdas = [lambda_1, lambda_2, lambda_3]\n",
    "        sum_log_prob = 0.0\n",
    "        for s in valid:\n",
    "            sum_log_prob += sentence_log_probability_with_linear_interpolation(s, n_unigrams, ngrams_list, lambdas)\n",
    "        if sum_log_prob > best_log_prob:\n",
    "            best_log_prob = sum_log_prob\n",
    "            best_lambda_1, best_lambda_2, best_lambda_3 = tuple(lambdas)\n",
    "        if verbose:\n",
    "            print \"current best log_prob: {}\".format(best_log_prob)\n",
    "            \n",
    "    return [best_lambda_1, best_lambda_2, best_lambda_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current best log_prob: -582.086646696\n",
      "current best log_prob: -582.086646696\n",
      "current best log_prob: -582.086646696\n",
      "current best log_prob: -582.086646696\n",
      "current best log_prob: -582.086646696\n",
      "current best log_prob: -574.619657516\n",
      "current best log_prob: -574.619657516\n",
      "current best log_prob: -574.619657516\n",
      "current best log_prob: -574.619657516\n",
      "current best log_prob: -574.619657516\n"
     ]
    }
   ],
   "source": [
    "best_lambdas = random_search_lambdas(valid, 10, n_unigrams, ngrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8863844098841657, 0.8442448614404383, -0.730629271324604]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in test: 134443\n",
      "Perplexity for smoothed trigram language model: 1.00433712579\n"
     ]
    }
   ],
   "source": [
    "n_words_in_test = compute_total_words(test)\n",
    "print \"Number of words in test: {}\".format(n_words_in_test)\n",
    "sum_log_prob = 0.0\n",
    "for s in test:\n",
    "    sum_log_prob += sentence_log_probability_with_linear_interpolation(s, n_unigrams, ngrams_list, best_lambdas)\n",
    "perplexity = 2**(-sum_log_prob/n_words_in_test)\n",
    "    \n",
    "print \"Perplexity for smoothed trigram language model: {}\".format(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐马尔可夫过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['我', '是', '中国人']\n",
    "emission_prob = {\n",
    "    ('N', '我'): 0.01,\n",
    "    ('N', '中国人'): 0.002,\n",
    "    ('V', '是'): 0.05\n",
    "}\n",
    "transition_prob = {\n",
    "    ('*', 'N'): 0.02,\n",
    "    ('*', 'V'): 0.0009,\n",
    "    ('N', 'V'): 0.05,\n",
    "    ('V', 'N'): 0.04\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_taggers = 2\n",
    "tagger_encoder = LabelEncoder()\n",
    "encoded_taggers = tagger_encoder.fit_transform(['N', 'V'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-93af2a5806c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdynamic_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_taggers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_taggers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtagger_to_tagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mword_on_tagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtagger_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdynamic_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransition_prob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagger_to_tagger\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0memission_prob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_on_tagger\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "dynamic_table = np.ones([n_taggers, len(sentence)], dtype=np.float32)*(-1)\n",
    "for j in range(n_taggers):\n",
    "    tagger_to_tagger = ('*', tagger_encoder.inverse_transform(j))\n",
    "    word_on_tagger = (tagger_encoder.inverse_transform(j), sentence[0])\n",
    "    dynamic_table[j, 0] = transition_prob.get(tagger_to_tagger,0.0)*emission_prob.get(word_on_tagger,0.0)\n",
    "for i in range(1, len(sentence)):\n",
    "    for j in range(n_taggers):\n",
    "        word_on_tagger = (tagger_encoder.inverse_transform(j),sentence[i])\n",
    "        max_at_i = 0.0\n",
    "        # loop all states in previous step\n",
    "        for k in range(n_taggers):\n",
    "            tagger_to_tagger = (tagger_encoder.inverse_transform(k),tagger_encoder.inverse_transform(j))\n",
    "            prob_k_j = dynamic_table[k, i-1]*transition_prob.get(tagger_to_tagger,0.0)\n",
    "            if max_at_i < prob_k_j:\n",
    "                max_at_i = prob_k_j\n",
    "        dynamic_table[j, i] = max_at_i\n",
    "        tmp = dynamic_table[j, i]\n",
    "        dynamic_table[j, i] = dynamic_table[j, i]*emission_prob.get(word_on_tagger, 0.0)\n",
    "\n",
    "back_pointer = np.ones((len(sentence, )), dtype=np.int32)*(-1)\n",
    "for i in range(len(sentence) - 1, -1, -1):\n",
    "    max_at_i = 0.0\n",
    "    max_j = 0.0\n",
    "    for j in range(n_taggers):\n",
    "        if dynamic_table[j, i] > max_at_i:\n",
    "            max_at_i = dynamic_table[j, i]\n",
    "            max_j = j\n",
    "            back_pointer[i] = j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use nltk to build Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = treebank.tagged_sents()\n",
    "\n",
    "# print train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-55ab737e3a71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHiddenMarkovModelTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_supervised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger = trainer.train_supervised(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27know",
   "language": "python",
   "name": "py27know"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
